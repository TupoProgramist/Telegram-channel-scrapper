{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from extraction_schemas import OpportunitySchema\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY\"  # Replace with your actual Groq API key\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.5,\n",
    "    max_retries=4,\n",
    "    disable_streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", encoding=\"utf-8\") as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import List, TypedDict\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Example(TypedDict):\n",
    "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
    "\n",
    "    For extraction, the tool calls are represented as instances of pydantic model.\n",
    "    \"\"\"\n",
    "\n",
    "    input: str  # This is the example text\n",
    "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
    "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
    "\n",
    "    This code is an adapter that converts our example to a list of messages\n",
    "    that can be fed into a chat model.\n",
    "\n",
    "    The list of messages per example corresponds to:\n",
    "\n",
    "    1) HumanMessage: contains the content from which content should be extracted.\n",
    "    2) AIMessage: contains the extracted information from the model\n",
    "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
    "\n",
    "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
    "    rather than for an extraction use case.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    # The name of the function right now corresponds\n",
    "                    # to the name of the pydantic model\n",
    "                    # This is implicit in the API right now,\n",
    "                    # and will be improved over time.\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import json\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. Your task is to extract only the relevant information \"\n",
    "            \"from the provided text. For each attribute, extract the value based on the information available \"\n",
    "            \"in the text. If an attribute's value is not explicitly mentioned or cannot be determined, return \"\n",
    "            \"null or empty list (depends on attribut description). Ensure all dates and times are formatted as dd.mm.yy hh:mm and that \"\n",
    "            \"extracted information is written in **perfect Ukrainian** without any spelling or grammatical errors. \"\n",
    "            \"Proper names, such as names of people, organizations, and locations, should be extracted as-is and not translated. \"\n",
    "            \"Only provide the requested fields.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\"examples\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "with open('examples.json', 'r', encoding='utf-8') as f:\n",
    "    examples_json = json.load(f)\n",
    "    for ex in examples_json[\"examples\"]:\n",
    "        examples.append((ex[\"input\"], OpportunitySchema.model_validate(ex[\"output\"])))\n",
    "\n",
    "messages = []\n",
    "\n",
    "for text, tool_call in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | llm.with_structured_output(schema=OpportunitySchema, include_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing message 0\n",
      "Error processing message 0: 'text'\n",
      "Processing message 1\n",
      "Error processing message 1: 'text'\n",
      "Processing message 2\n",
      "Error processing message 2: 'text'\n",
      "Processing message 3\n",
      "Error processing message 3: 'text'\n",
      "Processing message 4\n",
      "Error processing message 4: 'text'\n",
      "Processing message 5\n",
      "Error processing message 5: 'text'\n",
      "Processing message 6\n",
      "Error processing message 6: 'text'\n",
      "Processing message 7\n",
      "Error processing message 7: 'text'\n",
      "Processing message 8\n",
      "Error processing message 8: 'text'\n",
      "Processing message 9\n",
      "Error processing message 9: 'text'\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import JSON, display\n",
    "\n",
    "with open(\"test.json\", encoding=\"utf-8\") as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "\n",
    "responses = []\n",
    "for i, msg in enumerate(d[\"messages\"]):\n",
    "    try:\n",
    "        print(f\"Processing message {i}\")\n",
    "        resp = runnable.invoke({\"text\": msg[\"text\"], \"examples\": messages})\n",
    "        responses.append(resp)\n",
    "        display(JSON(resp[\"parsed\"].dict()))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing message {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
